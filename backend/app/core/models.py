from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_cohere import ChatCohere
from langchain_anthropic import ChatAnthropic
from langchain_deepseek import ChatDeepSeek
from app.core.config import Config

def ask_deepseek(prompt, temperature, max_tokens):

    """
    Sends a prompt to the DeepSeek Turbo model and returns the AI response.

    Parameters:
    prompt (str): The input prompt to send to the DeepSeek Turbo model.
    temperature (float): The temperature parameter controls the randomness of the output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
    max_tokens (int): The maximum number of tokens in the response. This parameter can be used to limit the length of the generated text.

    Returns:
    str: The AI response generated by the DeepSeek Turbo model.
    """

    llm = ChatDeepSeek(api_key=Config.DEEPSEEK_API_KEY , temperature=temperature , max_tokens=max_tokens,model="deepseek-chat")
    AI_Response = llm.invoke(prompt)
    return AI_Response.content

def ask_openai(prompt, temperature, max_tokens):

    """
    Sends a prompt to the GPT-3.5 Turbo model and returns the AI response.

    Parameters:
    prompt (str): The input prompt to send to the GPT-3.5 Turbo model.
    temperature (float): The temperature parameter controls the randomness of the output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
    max_tokens (int): The maximum number of tokens in the response. This parameter can be used to limit the length of the generated text.

    Returns:
    str: The AI response generated by the GPT-3.5 Turbo model.
    """

    llm = ChatOpenAI(api_key=Config.OPENAI_API_KEY, temperature=temperature, max_tokens=max_tokens, model="gpt-3.5-turbo")
    AI_Response = llm.invoke(prompt)
    return AI_Response.content

def ask_gemini(prompt, temperature):

    """
    Sends a prompt to the Gemini AI model and returns the response.

    Args:
        prompt (str): The input prompt to send to the AI model.
        temperature (float): The temperature parameter for controlling the randomness of the AI's response.

    Returns:
        str: The response generated by the AI model.
    """

    llm = ChatGoogleGenerativeAI(google_api_key=Config.GOOGLE_AI_API_KEY, temperature=temperature, model="gemini-pro")
    AI_Response = llm.invoke(prompt)
    return AI_Response.content

def ask_claude(prompt, temperature, max_tokens):

    """
    Sends a prompt to the Claude-2.1 model and returns the response.

    Args:
        prompt (str): The input prompt for the model.
        temperature (float): The temperature parameter for controlling the randomness of the model's output.
        max_tokens (int): The maximum number of tokens in the generated response.

    Returns:
        str: The response generated by the Claude-2.1 model.
    """

    llm = ChatAnthropic(anthropic_api_key=Config.ANTHROPIC_API_KEY, temperature=temperature, max_tokens=max_tokens, model_name="claude-2.1")
    AI_Response = llm.invoke(prompt)
    return AI_Response.content

def ask_command(prompt, temperature, max_tokens):

    """
    Sends a prompt to the ChatCohere model to generate a response.

    Args:
        prompt (str): The input prompt for the model.
        temperature (float): Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more deterministic.
        max_tokens (int): The maximum number of tokens in the generated response.

    Returns:
        str: The generated response from the ChatCohere model.
    """

    llm = ChatCohere(cohere_api_key=Config.COHERE_API_KEY, temperature=temperature, max_tokens=max_tokens, model="command-r-plus-08-2024")
    AI_Response = llm.invoke(prompt)
    return AI_Response.content
